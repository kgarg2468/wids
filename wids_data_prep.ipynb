{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# WiDS 2026 Data Preparation Notebook\n",
        "\n",
        "What this notebook does:\n",
        "1. Loads raw files from `data/`\n",
        "2. Explores the data with simple charts\n",
        "3. Cleans missing values and handles extreme values\n",
        "4. Builds new useful features\n",
        "5. Removes redundant features\n",
        "6. Creates train and validation splits\n",
        "7. Scales features for models that need scaling\n",
        "8. Saves all processed files for modeling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and imports\n",
        "\n",
        "We import common data science libraries and define a few constants used in the rest of the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If you are in Colab and packages are missing, uncomment and run this line.\n",
        "# !pip install -q pandas numpy matplotlib seaborn scikit-learn\n",
        "\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.set_option('display.max_columns', 200)\n",
        "sns.set_theme(style='whitegrid')\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "DATA_DIR = Path('data')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load data\n",
        "\n",
        "We load train, test, metadata, and sample submission files. Then we print shapes, data types, and sample rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_raw = pd.read_csv(DATA_DIR / 'train.csv')\n",
        "test_raw = pd.read_csv(DATA_DIR / 'test.csv')\n",
        "submission_template = pd.read_csv(DATA_DIR / 'sample_submission.csv')\n",
        "metadata = pd.read_csv(DATA_DIR / 'metaData.csv')\n",
        "\n",
        "print('Train shape:', train_raw.shape)\n",
        "print('Test shape:', test_raw.shape)\n",
        "print('Submission template shape:', submission_template.shape)\n",
        "print('Metadata shape:', metadata.shape)\n",
        "\n",
        "id_col = 'event_id'\n",
        "target_cols = ['time_to_hit_hours', 'event']\n",
        "base_feature_cols = [c for c in train_raw.columns if c not in [id_col] + target_cols]\n",
        "\n",
        "print('')\n",
        "print('Number of base features:', len(base_feature_cols))\n",
        "print('Base feature columns:')\n",
        "print(base_feature_cols)\n",
        "\n",
        "for name, df in [('train', train_raw), ('test', test_raw), ('sample_submission', submission_template), ('metaData', metadata)]:\n",
        "    print('')\n",
        "    print(f\"{name.upper()} - dtypes\")\n",
        "    print(df.dtypes)\n",
        "    print('')\n",
        "    print(f\"{name.upper()} - first 5 rows\")\n",
        "    display(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### What to look for\n",
        "\n",
        "The key thing to confirm here is that train has target columns (`time_to_hit_hours`, `event`) and test does not. The feature columns should match between train and test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Exploratory data analysis\n",
        "\n",
        "These charts help us understand class balance, missing data, feature shapes, and train versus test differences.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 Target distribution\n",
        "\n",
        "We compare how `time_to_hit_hours` looks for events that did happen versus censored events, and we check the event ratio.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "for ev, color, label in [(1, '#d95f02', 'Event happened (event=1)'), (0, '#1b9e77', 'Censored (event=0)')]:\n",
        "    subset = train_raw.loc[train_raw['event'] == ev, 'time_to_hit_hours']\n",
        "    axes[0].hist(subset, bins=20, alpha=0.65, color=color, label=label)\n",
        "\n",
        "axes[0].set_title('Distribution of time_to_hit_hours by event status')\n",
        "axes[0].set_xlabel('time_to_hit_hours')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].legend()\n",
        "\n",
        "train_raw['event'].value_counts().sort_index().plot(kind='bar', ax=axes[1], color=['#1b9e77', '#d95f02'])\n",
        "axes[1].set_title('Event indicator counts (censoring ratio)')\n",
        "axes[1].set_xlabel('event (0=censored, 1=hit)')\n",
        "axes[1].set_ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "event_rate = train_raw['event'].mean()\n",
        "print(f'Event rate (event=1): {event_rate:.3f}')\n",
        "print(f'Censoring rate (event=0): {1 - event_rate:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insight\n",
        "\n",
        "If one class is much larger than the other, model evaluation can be misleading. That is why we will stratify the train and validation split by `event` later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Missing values\n",
        "\n",
        "We calculate missing value percentages in train and test and plot them side by side.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_train_pct = train_raw.isna().mean().mul(100)\n",
        "missing_test_pct = test_raw.isna().mean().mul(100)\n",
        "\n",
        "missing_compare = pd.DataFrame({\n",
        "    'train_missing_pct': missing_train_pct,\n",
        "    'test_missing_pct': missing_test_pct.reindex(train_raw.columns, fill_value=np.nan)\n",
        "}).fillna(0).sort_values('train_missing_pct', ascending=False)\n",
        "\n",
        "display(missing_compare.head(15))\n",
        "\n",
        "plot_df = missing_compare[(missing_compare['train_missing_pct'] > 0) | (missing_compare['test_missing_pct'] > 0)]\n",
        "\n",
        "if len(plot_df) > 0:\n",
        "    ax = plot_df.plot(kind='bar', figsize=(14, 5), color=['#4c72b0', '#dd8452'])\n",
        "    ax.set_title('Missing value percentage by column')\n",
        "    ax.set_xlabel('Column')\n",
        "    ax.set_ylabel('Missing percentage')\n",
        "    plt.xticks(rotation=75, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "else:\n",
        "    print('No missing values found in train or test.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insight\n",
        "\n",
        "Columns with small missing percentages can be filled safely. Columns with very high missing percentages should be reviewed carefully before modeling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Feature distributions\n",
        "\n",
        "We plot histograms for all base features to see scale differences and skewed variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_features = len(base_feature_cols)\n",
        "n_cols = 4\n",
        "n_rows = int(np.ceil(num_features / n_cols))\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, 4 * n_rows))\n",
        "axes = np.array(axes).reshape(-1)\n",
        "\n",
        "for i, col in enumerate(base_feature_cols):\n",
        "    sns.histplot(train_raw[col], bins=20, kde=False, color='#4c72b0', ax=axes[i])\n",
        "    axes[i].set_title(col)\n",
        "    axes[i].set_xlabel(col)\n",
        "    axes[i].set_ylabel('Count')\n",
        "\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insight\n",
        "\n",
        "Many wildfire variables are usually skewed, which is normal for growth and distance data. Later we add transformed features to help models read these patterns better.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Correlation analysis\n",
        "\n",
        "We inspect feature correlations with target columns and flag very similar feature pairs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corr_matrix = train_raw[base_feature_cols + target_cols].corr(numeric_only=True)\n",
        "\n",
        "plt.figure(figsize=(10, 12))\n",
        "sns.heatmap(corr_matrix[[\"time_to_hit_hours\", \"event\"]].sort_values('event', ascending=False),\n",
        "            cmap='coolwarm', center=0)\n",
        "plt.title('Feature correlation with time_to_hit_hours and event')\n",
        "plt.xlabel('Targets')\n",
        "plt.ylabel('Features')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "feature_corr_abs = train_raw[base_feature_cols].corr(numeric_only=True).abs()\n",
        "upper = feature_corr_abs.where(np.triu(np.ones(feature_corr_abs.shape), k=1).astype(bool))\n",
        "high_corr_pairs = [\n",
        "    (c1, c2, upper.loc[c1, c2])\n",
        "    for c1 in upper.index\n",
        "    for c2 in upper.columns\n",
        "    if pd.notna(upper.loc[c1, c2]) and upper.loc[c1, c2] > 0.90\n",
        "]\n",
        "\n",
        "high_corr_df = pd.DataFrame(high_corr_pairs, columns=['feature_1', 'feature_2', 'abs_corr'])    .sort_values('abs_corr', ascending=False)\n",
        "\n",
        "print(f'Number of feature pairs with |r| > 0.90: {len(high_corr_df)}')\n",
        "display(high_corr_df.head(20))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insight\n",
        "\n",
        "If two features carry almost the same information, keeping both can make some models unstable. We will remove very redundant pairs later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.5 Train versus test distribution check\n",
        "\n",
        "We compare train and test for 10 features with the largest standardized mean difference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mean_shift = ((train_raw[base_feature_cols].mean() - test_raw[base_feature_cols].mean()).abs() /\n",
        "              (train_raw[base_feature_cols].std().replace(0, np.nan))).fillna(0)\n",
        "top_shift_features = mean_shift.sort_values(ascending=False).head(10).index.tolist()\n",
        "\n",
        "print('Top 10 potential shift features:')\n",
        "print(top_shift_features)\n",
        "\n",
        "fig, axes = plt.subplots(2, 5, figsize=(22, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(top_shift_features):\n",
        "    sns.kdeplot(train_raw[col], ax=axes[idx], label='Train', color='#4c72b0', fill=True, alpha=0.2)\n",
        "    sns.kdeplot(test_raw[col], ax=axes[idx], label='Test', color='#dd8452', fill=True, alpha=0.2)\n",
        "    axes[idx].set_title(col)\n",
        "    axes[idx].set_xlabel(col)\n",
        "    axes[idx].set_ylabel('Density')\n",
        "    if idx == 0:\n",
        "        axes[idx].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insight\n",
        "\n",
        "If train and test curves are very different for a feature, that feature may behave differently at prediction time. This is a useful warning for model tuning and validation strategy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Data cleaning\n",
        "\n",
        "We define reusable cleaning functions and create preview cleaned datasets. Final leak-safe fitting of cleaning parameters is done in the split section.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.1 Build cleaning helpers and review missing counts\n",
        "\n",
        "Rules:\n",
        "1. Missing below 5 percent: fill with train median\n",
        "2. Missing above 20 percent: flag for review\n",
        "3. Outliers: clip to 1st and 99th percentile, do not remove rows\n",
        "4. Drop zero-variance features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fit_cleaning_params(train_df, feature_cols):\n",
        "    params = {}\n",
        "\n",
        "    medians = train_df[feature_cols].median(numeric_only=True)\n",
        "    missing_pct = train_df[feature_cols].isna().mean()\n",
        "    high_missing_cols = missing_pct[missing_pct > 0.20].index.tolist()\n",
        "\n",
        "    numeric_cols = train_df[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "    clip_bounds = {}\n",
        "    for col in numeric_cols:\n",
        "        lower = train_df[col].quantile(0.01)\n",
        "        upper = train_df[col].quantile(0.99)\n",
        "        clip_bounds[col] = (lower, upper)\n",
        "\n",
        "    zero_var_cols = [col for col in feature_cols if train_df[col].nunique(dropna=False) <= 1]\n",
        "\n",
        "    outlier_flags = {}\n",
        "    for col in numeric_cols:\n",
        "        q1 = train_df[col].quantile(0.25)\n",
        "        q3 = train_df[col].quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        if iqr == 0:\n",
        "            outlier_flags[col] = 0\n",
        "            continue\n",
        "        lower = q1 - 3 * iqr\n",
        "        upper = q3 + 3 * iqr\n",
        "        outlier_flags[col] = int(((train_df[col] < lower) | (train_df[col] > upper)).sum())\n",
        "\n",
        "    params['medians'] = medians.to_dict()\n",
        "    params['high_missing_cols'] = high_missing_cols\n",
        "    params['numeric_cols'] = numeric_cols\n",
        "    params['clip_bounds'] = clip_bounds\n",
        "    params['zero_var_cols'] = zero_var_cols\n",
        "    params['outlier_flags'] = outlier_flags\n",
        "    return params\n",
        "\n",
        "\n",
        "def apply_cleaning(df, feature_cols, params, include_event=False):\n",
        "    cleaned = df.copy()\n",
        "\n",
        "    # Fill missing values with training medians\n",
        "    for col in feature_cols:\n",
        "        if col in params['medians']:\n",
        "            cleaned[col] = cleaned[col].fillna(params['medians'][col])\n",
        "\n",
        "    # Winsorization using train-based percentiles\n",
        "    for col in params['numeric_cols']:\n",
        "        if col in cleaned.columns:\n",
        "            lo, hi = params['clip_bounds'][col]\n",
        "            cleaned[col] = cleaned[col].clip(lo, hi)\n",
        "\n",
        "    # Remove zero variance features\n",
        "    drop_cols = [c for c in params['zero_var_cols'] if c in cleaned.columns]\n",
        "    if drop_cols:\n",
        "        cleaned = cleaned.drop(columns=drop_cols)\n",
        "\n",
        "    # Enforce integer dtypes for indicator columns\n",
        "    if 'low_temporal_resolution_0_5h' in cleaned.columns:\n",
        "        cleaned['low_temporal_resolution_0_5h'] = cleaned['low_temporal_resolution_0_5h'].round().astype(int)\n",
        "\n",
        "    if include_event and 'event' in cleaned.columns:\n",
        "        cleaned['event'] = cleaned['event'].round().astype(int)\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "missing_counts = pd.DataFrame({\n",
        "    'train_missing_count': train_raw.isna().sum(),\n",
        "    'test_missing_count': test_raw.isna().sum().reindex(train_raw.columns, fill_value=np.nan)\n",
        "}).fillna(0).astype(int)\n",
        "\n",
        "display(missing_counts[missing_counts.sum(axis=1) > 0].sort_values('train_missing_count', ascending=False).head(20))\n",
        "\n",
        "cleaning_preview_params = fit_cleaning_params(train_raw, base_feature_cols)\n",
        "\n",
        "print('Columns with >20% missing in train:', cleaning_preview_params['high_missing_cols'])\n",
        "print('Zero variance columns:', cleaning_preview_params['zero_var_cols'])\n",
        "\n",
        "outlier_flag_df = pd.DataFrame({\n",
        "    'feature': list(cleaning_preview_params['outlier_flags'].keys()),\n",
        "    'outlier_count_3xIQR': list(cleaning_preview_params['outlier_flags'].values())\n",
        "}).sort_values('outlier_count_3xIQR', ascending=False)\n",
        "\n",
        "display(outlier_flag_df.head(15))\n",
        "\n",
        "train_clean_preview = apply_cleaning(train_raw, base_feature_cols, cleaning_preview_params, include_event=True)\n",
        "test_clean_preview = apply_cleaning(test_raw, base_feature_cols, cleaning_preview_params, include_event=False)\n",
        "\n",
        "clean_base_feature_cols = [c for c in base_feature_cols if c not in cleaning_preview_params['zero_var_cols']]\n",
        "print('Base feature count after zero-variance removal:', len(clean_base_feature_cols))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insight\n",
        "\n",
        "The cleaning rules are simple and repeatable. This makes the pipeline easier to audit and less likely to break when rerun.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature engineering\n",
        "\n",
        "We create extra features that combine growth, distance, direction, and time signals.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.1 Create engineered features for train and test\n",
        "\n",
        "Each engineered feature is created with the same formula in every dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def safe_divide(a, b):\n",
        "    return a / (b.replace(0, np.nan) if isinstance(b, pd.Series) else (b if b != 0 else np.nan))\n",
        "\n",
        "\n",
        "def engineer_features(df):\n",
        "    out = df.copy()\n",
        "\n",
        "    # Cyclical encodings for temporal columns\n",
        "    cyclical_specs = [\n",
        "        ('event_start_hour', 24),\n",
        "        ('event_start_dayofweek', 7),\n",
        "        ('event_start_month', 12),\n",
        "    ]\n",
        "\n",
        "    for col, period in cyclical_specs:\n",
        "        if col in out.columns:\n",
        "            out[f'{col}_sin'] = np.sin(2 * np.pi * out[col] / period)\n",
        "            out[f'{col}_cos'] = np.cos(2 * np.pi * out[col] / period)\n",
        "            out = out.drop(columns=[col])\n",
        "\n",
        "    # Interaction features\n",
        "    out['threat_score'] = out['closing_speed_m_per_h'] * out['alignment_abs']\n",
        "    out['growth_threat'] = out['area_growth_rate_ha_per_h'] * out['closing_speed_m_per_h']\n",
        "    out['directional_advance'] = out['along_track_speed'] * out['alignment_cos']\n",
        "    out['area_distance_ratio'] = safe_divide(out['area_first_ha'], out['dist_min_ci_0_5h'] + 1).fillna(0)\n",
        "    out['speed_per_area'] = safe_divide(out['centroid_speed_m_per_h'], out['area_first_ha'] + 1).fillna(0)\n",
        "    out['radial_vs_centroid_speed'] = safe_divide(out['radial_growth_rate_m_per_h'], out['centroid_speed_m_per_h'] + 1).fillna(0)\n",
        "\n",
        "    # Nonlinear transforms\n",
        "    out['log_dist_min'] = np.log1p(np.clip(out['dist_min_ci_0_5h'], a_min=0, a_max=None))\n",
        "    out['sqrt_area_first'] = np.sqrt(np.clip(out['area_first_ha'], a_min=0, a_max=None))\n",
        "    out['closing_speed_sq'] = out['closing_speed_m_per_h'] ** 2\n",
        "\n",
        "    # Temporal resolution interactions\n",
        "    out['growth_quality'] = out['area_growth_rate_ha_per_h'] * (1 - out['low_temporal_resolution_0_5h'])\n",
        "    out['speed_quality'] = out['closing_speed_m_per_h'] * out['dist_fit_r2_0_5h']\n",
        "\n",
        "    # Survival-focused heuristics\n",
        "    out['time_to_close_estimate'] = safe_divide(out['dist_min_ci_0_5h'], out['closing_speed_abs_m_per_h'] + 1).fillna(200)\n",
        "    out['time_to_close_estimate'] = out['time_to_close_estimate'].clip(0, 200)\n",
        "\n",
        "    out['is_closing'] = (out['closing_speed_m_per_h'] > 0).astype(int)\n",
        "    out['is_aligned'] = (out['alignment_abs'] > 0.5).astype(int)\n",
        "    out['immediate_threat'] = ((out['is_closing'] == 1) & (out['is_aligned'] == 1)).astype(int)\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "train_fe_preview = engineer_features(train_clean_preview)\n",
        "test_fe_preview = engineer_features(test_clean_preview)\n",
        "\n",
        "id_and_targets = [id_col] + target_cols\n",
        "engineered_feature_cols = [\n",
        "    c for c in train_fe_preview.columns\n",
        "    if c not in train_clean_preview.columns and c not in id_and_targets\n",
        "]\n",
        "\n",
        "print('Engineered feature count:', len(engineered_feature_cols))\n",
        "print(engineered_feature_cols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Engineered feature summary table\n",
        "\n",
        "This quick table gives a plain-language reminder of what each new feature means.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "engineered_descriptions = {\n",
        "    'event_start_hour_sin': 'Circular encoding of start hour (sine)',\n",
        "    'event_start_hour_cos': 'Circular encoding of start hour (cosine)',\n",
        "    'event_start_dayofweek_sin': 'Circular encoding of day of week (sine)',\n",
        "    'event_start_dayofweek_cos': 'Circular encoding of day of week (cosine)',\n",
        "    'event_start_month_sin': 'Circular encoding of month (sine)',\n",
        "    'event_start_month_cos': 'Circular encoding of month (cosine)',\n",
        "    'threat_score': 'Closing speed times direction alignment',\n",
        "    'growth_threat': 'Growth rate times closing speed',\n",
        "    'directional_advance': 'Along-track speed weighted by direction sign',\n",
        "    'area_distance_ratio': 'Initial area divided by distance to evac zone',\n",
        "    'speed_per_area': 'Centroid speed relative to area',\n",
        "    'radial_vs_centroid_speed': 'Spread speed relative to centroid movement speed',\n",
        "    'log_dist_min': 'Log-scaled minimum distance to evac zone',\n",
        "    'sqrt_area_first': 'Square-root transform of initial area',\n",
        "    'closing_speed_sq': 'Squared closing speed to stress larger values',\n",
        "    'growth_quality': 'Growth rate discounted when temporal resolution is low',\n",
        "    'speed_quality': 'Closing speed weighted by linear-fit quality',\n",
        "    'time_to_close_estimate': 'Simple time estimate to reach evac zone',\n",
        "    'is_closing': 'Binary flag for positive closing speed',\n",
        "    'is_aligned': 'Binary flag for alignment above 0.5',\n",
        "    'immediate_threat': 'Binary flag when both closing and aligned',\n",
        "}\n",
        "\n",
        "feature_summary = pd.DataFrame([\n",
        "    {'feature': f, 'description': engineered_descriptions.get(f, 'Engineered feature')}\n",
        "    for f in engineered_feature_cols\n",
        "]).sort_values('feature')\n",
        "\n",
        "display(feature_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insight\n",
        "\n",
        "These engineered features package raw measurements into easier risk signals, such as \"moving closer quickly\" or \"large fire already near an evacuation zone\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Feature selection\n",
        "\n",
        "Now we remove highly redundant features and keep a cleaner feature set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 Correlation-based redundancy removal\n",
        "\n",
        "If two features have absolute correlation above 0.95, we keep the one that is more correlated with `event` in the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_features_by_correlation(train_df, candidate_cols, target_col='event', threshold=0.95):\n",
        "    corr_abs = train_df[candidate_cols].corr(numeric_only=True).abs()\n",
        "    upper = corr_abs.where(np.triu(np.ones(corr_abs.shape), k=1).astype(bool))\n",
        "\n",
        "    target_corr = train_df[candidate_cols].corrwith(train_df[target_col]).abs().fillna(0)\n",
        "\n",
        "    to_drop = set()\n",
        "    decisions = []\n",
        "\n",
        "    for col in upper.columns:\n",
        "        high_corr_with_col = upper.index[upper[col] > threshold].tolist()\n",
        "        for row_col in high_corr_with_col:\n",
        "            if row_col in to_drop or col in to_drop:\n",
        "                continue\n",
        "\n",
        "            row_score = target_corr.get(row_col, 0)\n",
        "            col_score = target_corr.get(col, 0)\n",
        "\n",
        "            if row_score < col_score:\n",
        "                drop_col, keep_col = row_col, col\n",
        "            elif col_score < row_score:\n",
        "                drop_col, keep_col = col, row_col\n",
        "            else:\n",
        "                drop_col, keep_col = sorted([row_col, col])[1], sorted([row_col, col])[0]\n",
        "\n",
        "            to_drop.add(drop_col)\n",
        "            decisions.append({\n",
        "                'feature_kept': keep_col,\n",
        "                'feature_dropped': drop_col,\n",
        "                'pair_abs_corr': upper.loc[row_col, col],\n",
        "                'kept_abs_corr_with_event': target_corr.get(keep_col, 0),\n",
        "                'dropped_abs_corr_with_event': target_corr.get(drop_col, 0),\n",
        "            })\n",
        "\n",
        "    selected = [c for c in candidate_cols if c not in to_drop]\n",
        "    decisions_df = pd.DataFrame(decisions).sort_values('pair_abs_corr', ascending=False) if decisions else pd.DataFrame()\n",
        "    return selected, decisions_df\n",
        "\n",
        "\n",
        "preview_candidate_cols = [c for c in train_fe_preview.columns if c not in [id_col] + target_cols]\n",
        "preview_feature_cols, preview_drop_log = select_features_by_correlation(\n",
        "    train_fe_preview,\n",
        "    preview_candidate_cols,\n",
        "    target_col='event',\n",
        "    threshold=0.95,\n",
        ")\n",
        "\n",
        "print('Candidate feature count before pruning:', len(preview_candidate_cols))\n",
        "print('Feature count after pruning:', len(preview_feature_cols))\n",
        "\n",
        "if len(preview_drop_log) > 0:\n",
        "    display(preview_drop_log.head(20))\n",
        "else:\n",
        "    print('No highly redundant pairs were removed at the 0.95 threshold.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insight\n",
        "\n",
        "This step keeps important information while reducing overlap. Fewer redundant columns can make models easier to train and explain.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Train and validation split\n",
        "\n",
        "This section builds final modeling datasets with leakage safeguards:\n",
        "1. Split raw train into train and validation with stratification\n",
        "2. Fit cleaning and feature selection only on train split\n",
        "3. Apply the same learned rules to validation and test\n",
        "4. Create 5-fold stratified CV indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Build leak-safe processed train, validation, and test datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Primary split on raw labeled data\n",
        "raw_train_split, raw_val_split = train_test_split(\n",
        "    train_raw,\n",
        "    test_size=0.2,\n",
        "    stratify=train_raw['event'],\n",
        "    random_state=RANDOM_STATE,\n",
        ")\n",
        "\n",
        "# Fit cleaning rules on training split only\n",
        "clean_params = fit_cleaning_params(raw_train_split, base_feature_cols)\n",
        "\n",
        "train_clean = apply_cleaning(raw_train_split, base_feature_cols, clean_params, include_event=True)\n",
        "val_clean = apply_cleaning(raw_val_split, base_feature_cols, clean_params, include_event=True)\n",
        "test_clean = apply_cleaning(test_raw, base_feature_cols, clean_params, include_event=False)\n",
        "\n",
        "# Apply engineering to all sets\n",
        "train_fe = engineer_features(train_clean)\n",
        "val_fe = engineer_features(val_clean)\n",
        "test_fe = engineer_features(test_clean)\n",
        "\n",
        "# Fit feature pruning on training split only\n",
        "train_candidate_cols = [c for c in train_fe.columns if c not in [id_col] + target_cols]\n",
        "feature_cols, drop_log = select_features_by_correlation(\n",
        "    train_fe,\n",
        "    train_candidate_cols,\n",
        "    target_col='event',\n",
        "    threshold=0.95,\n",
        ")\n",
        "\n",
        "# Final processed datasets\n",
        "train_df = train_fe[[id_col] + feature_cols + target_cols].copy()\n",
        "val_df = val_fe[[id_col] + feature_cols + target_cols].copy()\n",
        "test_df = test_fe[[id_col] + feature_cols].copy()\n",
        "\n",
        "# Combined processed train for CV folds (all labeled rows)\n",
        "processed_train = pd.concat([train_df, val_df], axis=0, ignore_index=True)\n",
        "\n",
        "# 5-fold stratified CV\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "folds = list(skf.split(processed_train[feature_cols], processed_train['event']))\n",
        "\n",
        "print('Final feature count:', len(feature_cols))\n",
        "print('Train shape:', train_df.shape)\n",
        "print('Validation shape:', val_df.shape)\n",
        "print('Test shape:', test_df.shape)\n",
        "\n",
        "print('')\n",
        "print('Dropped redundant features (first 15 rows):')\n",
        "if len(drop_log) > 0:\n",
        "    display(drop_log.head(15))\n",
        "else:\n",
        "    print('No features dropped in this stage.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Validation checks\n",
        "\n",
        "We verify stratification quality, overlap safety, shape consistency, and missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_event_rate = train_df['event'].mean()\n",
        "val_event_rate = val_df['event'].mean()\n",
        "event_rate_gap = abs(train_event_rate - val_event_rate)\n",
        "\n",
        "train_ids = set(train_df[id_col])\n",
        "val_ids = set(val_df[id_col])\n",
        "overlap_ids = train_ids.intersection(val_ids)\n",
        "\n",
        "assert event_rate_gap <= 0.05, f'Event rate gap too large: {event_rate_gap:.4f}'\n",
        "assert len(overlap_ids) == 0, 'Found overlapping event_id values between train and validation.'\n",
        "assert list(train_df[feature_cols].columns) == list(val_df[feature_cols].columns) == list(test_df[feature_cols].columns), 'Feature columns are not aligned.'\n",
        "assert train_df[feature_cols].isna().sum().sum() == 0, 'NaN found in train features.'\n",
        "assert val_df[feature_cols].isna().sum().sum() == 0, 'NaN found in validation features.'\n",
        "assert test_df[feature_cols].isna().sum().sum() == 0, 'NaN found in test features.'\n",
        "\n",
        "print('Event rate in train:', round(train_event_rate, 4))\n",
        "print('Event rate in validation:', round(val_event_rate, 4))\n",
        "print('Absolute event rate gap:', round(event_rate_gap, 4))\n",
        "print('Overlapping event_id count:', len(overlap_ids))\n",
        "print('Feature alignment check: PASSED')\n",
        "print('NaN checks: PASSED')\n",
        "print('Number of CV folds:', len(folds))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Insight\n",
        "\n",
        "This is the most important quality gate in the notebook. If these checks pass, the data is consistent for modeling and less likely to leak information across splits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature scaling\n",
        "\n",
        "Some models need standardized features. We fit `StandardScaler` on train only and apply it to validation and test.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.1 Fit and apply scaler\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "train_scaled = scaler.fit_transform(train_df[feature_cols])\n",
        "val_scaled = scaler.transform(val_df[feature_cols])\n",
        "test_scaled = scaler.transform(test_df[feature_cols])\n",
        "\n",
        "train_scaled_df = pd.DataFrame(train_scaled, columns=feature_cols, index=train_df.index)\n",
        "val_scaled_df = pd.DataFrame(val_scaled, columns=feature_cols, index=val_df.index)\n",
        "test_scaled_df = pd.DataFrame(test_scaled, columns=feature_cols, index=test_df.index)\n",
        "\n",
        "print('Scaled train shape:', train_scaled_df.shape)\n",
        "print('Scaled validation shape:', val_scaled_df.shape)\n",
        "print('Scaled test shape:', test_scaled_df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Save processed outputs\n",
        "\n",
        "We save unscaled files, scaled files, feature names, CV folds, and the scaler object for reuse.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 9.1 Write all output artifacts to `data/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Unscaled files\n",
        "train_df.to_csv(DATA_DIR / 'train_processed.csv', index=False)\n",
        "val_df.to_csv(DATA_DIR / 'val_processed.csv', index=False)\n",
        "test_df.to_csv(DATA_DIR / 'test_processed.csv', index=False)\n",
        "\n",
        "# Scaled files\n",
        "train_scaled_df.to_csv(DATA_DIR / 'train_scaled.csv', index=False)\n",
        "val_scaled_df.to_csv(DATA_DIR / 'val_scaled.csv', index=False)\n",
        "test_scaled_df.to_csv(DATA_DIR / 'test_scaled.csv', index=False)\n",
        "\n",
        "# Feature list\n",
        "with open(DATA_DIR / 'feature_names.json', 'w') as f:\n",
        "    json.dump(feature_cols, f, indent=2)\n",
        "\n",
        "# CV folds\n",
        "with open(DATA_DIR / 'cv_folds.pkl', 'wb') as f:\n",
        "    pickle.dump(folds, f)\n",
        "\n",
        "# Scaler object\n",
        "with open(DATA_DIR / 'standard_scaler.pkl', 'wb') as f:\n",
        "    pickle.dump(scaler, f)\n",
        "\n",
        "print('Saved files:')\n",
        "print('- data/train_processed.csv')\n",
        "print('- data/val_processed.csv')\n",
        "print('- data/test_processed.csv')\n",
        "print('- data/train_scaled.csv')\n",
        "print('- data/val_scaled.csv')\n",
        "print('- data/test_scaled.csv')\n",
        "print('- data/feature_names.json')\n",
        "print('- data/cv_folds.pkl')\n",
        "print('- data/standard_scaler.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Final summary and sanity checks\n",
        "\n",
        "We print final dataset stats and quick feature importance proxies using absolute correlation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 10.1 Final report\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "original_feature_count = len(base_feature_cols)\n",
        "final_feature_count = len(feature_cols)\n",
        "engineered_in_final = [c for c in feature_cols if c not in base_feature_cols]\n",
        "\n",
        "summary_rows = [\n",
        "    ('Original base feature count', original_feature_count),\n",
        "    ('Final selected feature count', final_feature_count),\n",
        "    ('Engineered features kept', len(engineered_in_final)),\n",
        "    ('Train rows', len(train_df)),\n",
        "    ('Validation rows', len(val_df)),\n",
        "    ('Test rows', len(test_df)),\n",
        "    ('Train event rate', round(train_df['event'].mean(), 4)),\n",
        "    ('Validation event rate', round(val_df['event'].mean(), 4)),\n",
        "]\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows, columns=['Metric', 'Value'])\n",
        "display(summary_df)\n",
        "\n",
        "corr_event = train_df[feature_cols + ['event']].corr(numeric_only=True)['event'].drop('event').abs().sort_values(ascending=False)\n",
        "corr_time = train_df[feature_cols + ['time_to_hit_hours']].corr(numeric_only=True)['time_to_hit_hours'].drop('time_to_hit_hours').abs().sort_values(ascending=False)\n",
        "\n",
        "print('Top 10 features by absolute correlation with event:')\n",
        "display(corr_event.head(10).to_frame('abs_corr_with_event'))\n",
        "\n",
        "print('Top 10 features by absolute correlation with time_to_hit_hours:')\n",
        "display(corr_time.head(10).to_frame('abs_corr_with_time_to_hit_hours'))\n",
        "\n",
        "no_nan_train = train_df[feature_cols].isna().sum().sum() == 0\n",
        "no_nan_val = val_df[feature_cols].isna().sum().sum() == 0\n",
        "no_nan_test = test_df[feature_cols].isna().sum().sum() == 0\n",
        "\n",
        "print('No NaN in train features:', no_nan_train)\n",
        "print('No NaN in validation features:', no_nan_val)\n",
        "print('No NaN in test features:', no_nan_test)\n",
        "print('Train and validation IDs disjoint:', set(train_df[id_col]).isdisjoint(set(val_df[id_col])))\n",
        "print('Matching feature columns across splits:', list(train_df[feature_cols].columns) == list(val_df[feature_cols].columns) == list(test_df[feature_cols].columns))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Done\n",
        "\n",
        "The processed files are now ready for modeling. You can use unscaled files for tree-based methods and scaled files for linear or neural models.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "wids_data_prep.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
